{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucía García-Duarte Sáenz\n",
    "# MFSTC-CNN-LSTM Model\n",
    "\n",
    "- Architecture: \n",
    "  1. User defined number of Conv2D followed by Maxpooling layers\n",
    "  2. User defined number of LSTM layers\n",
    "  3. Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Flatten, TimeDistributed, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "- P NxT matrices, which describes the ($T_1, \\cdots, T_t$) records over t timesteps for the N stations for each of the P variables.\n",
    "\n",
    "Data is recorded every hour over 37 stations for 26 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'C:/Users/lgarc/OneDrive/Documentos/AA UNI/Master/TFM/2. Missing values imputation/temporal imputed data/MOVING AVERAGE'\n",
    "\n",
    "data_temp = pd.read_csv(mypath + '/data_imputed_air_temp.csv')\n",
    "data_temp = data_temp.drop(columns=['Unnamed: 0','date'])\n",
    "data_temp = data_temp.T\n",
    "\n",
    "data_dew = pd.read_csv(mypath + '/data_imputed_dew_point.csv')\n",
    "data_dew = data_dew.drop(columns=['Unnamed: 0','date'])\n",
    "data_dew = data_dew.T\n",
    "\n",
    "data_wd = pd.read_csv(mypath + '/data_imputed_wd.csv')\n",
    "data_wd = data_wd.drop(columns=['Unnamed: 0','date'])\n",
    "data_wd = data_wd.T\n",
    "\n",
    "data_ws = pd.read_csv(mypath + '/data_imputed_ws.csv')\n",
    "data_ws = data_ws.drop(columns=['Unnamed: 0','date'])\n",
    "data_ws = data_ws.T\n",
    "\n",
    "data_vis = pd.read_csv(mypath + '/data_imputed_visibility.csv')\n",
    "data_vis = data_vis.drop(columns=['Unnamed: 0','date'])\n",
    "data_vis = data_vis.T\n",
    "\n",
    "data_RH = pd.read_csv(mypath + '/data_imputed_RH.csv')\n",
    "data_RH = data_RH.drop(columns=['Unnamed: 0','date'])\n",
    "data_RH = data_RH.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join data\n",
    "\n",
    "data_all = np.array([data_temp.values, data_dew.values, data_wd.values, data_ws.values, data_vis.values,data_RH.values])\n",
    "data_all = data_all.reshape((1,6, 37, -1))\n",
    "data_all = data_all.T\n",
    "\n",
    "time_len, num_nodes, num_vars, aux = data_all.shape\n",
    "print(\"No. of stations:\", num_nodes, \"No. of variables:\", num_vars, \"\\nNo. of timesteps:\", time_len)\n",
    "\n",
    "data_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN/VAL/TEST SPLIT\n",
    "\n",
    "test_rate = 0.3\n",
    "valid_rate = 0.1\n",
    "\n",
    "time_len = data_all.shape[0]\n",
    "train_portion = 1 - valid_rate - test_rate\n",
    "train_size = int(time_len * train_portion)\n",
    "valid_size = int(time_len * valid_rate)\n",
    "\n",
    "train_data = data_all[:train_size]\n",
    "valid_data = data_all[train_size:train_size+valid_size]\n",
    "test_data = data_all[train_size+valid_size:]\n",
    "    \n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCALE DATA\n",
    "\n",
    "def scale_data(train_data, valid_data, test_data):\n",
    "    \n",
    "    max_speed = np.reshape(np.squeeze(train_data).max(0).max(0), (6, 1))\n",
    "    min_speed = np.reshape(np.squeeze(train_data).min(0).min(0), (6, 1))\n",
    "    \n",
    "    train_scaled = (train_data - min_speed) / (max_speed - min_speed)\n",
    "    valid_scaled = (valid_data - min_speed) / (max_speed - min_speed)\n",
    "    test_scaled = (test_data - min_speed) / (max_speed - min_speed)\n",
    "    \n",
    "    return train_scaled, valid_scaled, test_scaled\n",
    "\n",
    "train_scaled, valid_scaled, test_scaled = scale_data(train_data, valid_data, test_data)\n",
    "#del data_all # to save memory for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPARE DATA\n",
    "\n",
    "seq_len = 24    # change by 24, 24*7, 24*7*2, 24*7*3 (h)\n",
    "pre_len = 1     # change by 1, 2, 3 (h)\n",
    "\n",
    "class CustomGen(TimeseriesGenerator):\n",
    "    def __getitem__(self, idx):\n",
    "        x,y = super().__getitem__(idx)\n",
    "        y = y[:,:,0,:] # modify the generator to get only temperature data in the output\n",
    "        return x, y\n",
    "\n",
    "if pre_len == 1:\n",
    "    train_generator = CustomGen(train_scaled, train_scaled, length=seq_len, batch_size = 1)\n",
    "    valid_generator = CustomGen(valid_scaled, valid_scaled, length=seq_len, batch_size = 1)\n",
    "    test_generator = CustomGen(test_scaled, test_scaled, length=seq_len, batch_size = 1)\n",
    "else:\n",
    "    train_generator = CustomGen(train_scaled[:-pre_len+1,:,:,:], train_scaled[pre_len-1:,:,:,:], length=seq_len, batch_size = 1)\n",
    "    valid_generator = CustomGen(valid_scaled[:-pre_len+1,:,:,:], valid_scaled[pre_len-1:,:,:,:], length=seq_len, batch_size = 1)\n",
    "    test_generator = CustomGen(test_scaled[:-pre_len+1,:,:,:], test_scaled[pre_len-1:,:,:,:], length=seq_len, batch_size = 1)\n",
    "\n",
    "x, y = train_generator[0]\n",
    "print('Train: %d' % len(train_generator))\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "x, y = valid_generator[0]\n",
    "print('Valid: %d' % len(valid_generator))\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "x, y = test_generator[0]\n",
    "print('Test: %d' % len(test_generator))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session() # clear previous model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3), activation='relu', padding = 'same'), input_shape=(None, num_nodes, num_vars, 1)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size = 2, padding = 'same')))\n",
    "model.add(TimeDistributed(Conv2D(64, (5,3), activation='relu', padding = 'same')))\n",
    "model.add(TimeDistributed(Flatten())) \n",
    "model.add(TimeDistributed(Dense(128, activation='relu'))) \n",
    "\n",
    "model.add(LSTM(16, activation='tanh', return_sequences=False)) # change by 16, 32, 64, 128, 256\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dense(num_nodes, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae',metrics=[\"mse\"])\n",
    "\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "#mc = ModelCheckpoint('models_MFSTC-CNN-LSTM/24_1_32_16/best_model.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = int(len(train_generator) / 32)\n",
    "validation_steps = int(len(valid_generator) / 32)\n",
    "\n",
    "train_start = datetime.datetime.now()\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    shuffle=True,\n",
    "    #verbose=0,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=validation_steps#, callbacks = [mc]\n",
    ")\n",
    "train_end = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['mse'])\n",
    "plt.plot(history.history['val_mse'])\n",
    "plt.ylabel('mse')\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Train loss: \",\n",
    "    history.history[\"loss\"][-1],\n",
    "    \"\\nValid loss:\",\n",
    "    history.history[\"val_loss\"][-1],\n",
    "    \"\\nElapsed time: \",\n",
    "    train_end-train_start,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##If loading best model\n",
    "#from tensorflow import keras\n",
    "#model=keras.models.load_model(\"models_MFSTC-CNN-LSTM/24_1_32_16/best_model.h5\")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ythat = model.predict(train_generator)\n",
    "yhat = model.predict(test_generator)\n",
    "yht = model.predict(valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = []\n",
    "for i in range(len(train_generator)):\n",
    "    x, y = train_generator[i]\n",
    "    trainY.append(y[0])\n",
    "\n",
    "trainY = np.squeeze(np.array(trainY))\n",
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY = []\n",
    "for i in range(len(test_generator)):\n",
    "    x, y = test_generator[i]\n",
    "    testY.append(y[0])\n",
    "\n",
    "testY = np.squeeze(np.array(testY))\n",
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validY = []\n",
    "for i in range(len(valid_generator)):\n",
    "    x, y = valid_generator[i]\n",
    "    validY.append(y[0])\n",
    "\n",
    "validY = np.squeeze(np.array(validY))\n",
    "validY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rescale values\n",
    "max_speed = np.reshape(np.squeeze(train_data).max(0).max(0), (6, 1))[0]\n",
    "min_speed = np.reshape(np.squeeze(train_data).min(0).min(0), (6, 1))[0]\n",
    "\n",
    "## actual values\n",
    "train_rescref = np.array(trainY * max_speed)\n",
    "test_rescref = np.array(testY * max_speed)\n",
    "valid_rescref = np.array(validY * max_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rescpred = np.array((ythat) * max_speed)\n",
    "test_rescpred = np.array((yhat) * max_speed)\n",
    "valid_rescpred = np.array((yht) * max_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the naïve values (beware to change the files depending on the pre_len value)\n",
    "trainnpredc = pd.read_csv('C:/Users/lgarc/OneDrive/Documentos/AA UNI/Master/TFM/3. Review of the state of the art/trainnpredc_2.txt', sep=' ', header=None).to_numpy()\n",
    "testnpredc = pd.read_csv('C:/Users/lgarc/OneDrive/Documentos/AA UNI/Master/TFM/3. Review of the state of the art/testnpredc_2.txt', sep=' ', header=None).to_numpy()\n",
    "validnpredc = pd.read_csv('C:/Users/lgarc/OneDrive/Documentos/AA UNI/Master/TFM/3. Review of the state of the art/validnpredc_2.txt', sep=' ', header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance measures\n",
    "\n",
    "from sklearn.metrics import r2_score, explained_variance_score#, mean_absolute_percentage_error\n",
    "from sklearn.metrics.regression import _check_reg_targets, check_consistent_length\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred,\n",
    "                                   sample_weight=None,\n",
    "                                   multioutput='uniform_average'):\n",
    "\n",
    "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
    "        y_true, y_pred, multioutput)\n",
    "    check_consistent_length(y_true, y_pred, sample_weight)\n",
    "    epsilon = np.finfo(np.float64).eps\n",
    "    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\n",
    "    output_errors = np.average(mape,\n",
    "                               weights=sample_weight, axis=0)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            return output_errors\n",
    "        elif multioutput == 'uniform_average':\n",
    "            # pass None as weights to np.average: uniform mean\n",
    "            multioutput = None\n",
    "\n",
    "    return np.average(output_errors, weights=multioutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "seg_nmael = [] #naïve mae\n",
    "seg_nmsel = [] #naïve mse\n",
    "seg_nmape = [] #naïve mape\n",
    "seg_nacc = []  #naïve accuracy\n",
    "seg_nR2 = []   #naïve coeff of determination\n",
    "seg_nvar = []  #naïve coeff of variance score\n",
    "\n",
    "seg_mael = []  #mae\n",
    "seg_masel = [] #mase\n",
    "seg_msel = []  #mse\n",
    "seg_mape = []  #mape\n",
    "seg_acc = []   #accuracy\n",
    "seg_R2 = []    #coeff of determination\n",
    "seg_var = []   #coeff of variance score\n",
    "\n",
    "kk=num_nodes\n",
    "for j in range(kk):\n",
    "    \n",
    "    ## NAIVE\n",
    "    # Mean Square Error \n",
    "    seg_nmsel.append(np.mean(np.square(train_rescref.T[j] - trainnpredc.T[j]))) \n",
    "        \n",
    "    # Mean Absolute Error \n",
    "    seg_nmael.append(np.mean(np.abs(train_rescref.T[j] - trainnpredc.T[j])))  \n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_nmape.append(mean_absolute_percentage_error(train_rescref.T[j],trainnpredc.T[j]))\n",
    "    \n",
    "    # Accuracy\n",
    "    seg_nacc.append(1-np.linalg.norm(train_rescref.T[j]-trainnpredc.T[j])/np.linalg.norm(train_rescref.T[j]))\n",
    "    \n",
    "    # Coefficient of determination\n",
    "    seg_nR2.append(r2_score(train_rescref.T[j],trainnpredc.T[j]))\n",
    "    \n",
    "    # Explained variance score\n",
    "    seg_nvar.append(explained_variance_score(train_rescref.T[j],trainnpredc.T[j]))\n",
    "    \n",
    "    \n",
    "    ## NN\n",
    "    # Mean Absolute Error \n",
    "    seg_mael.append(np.mean(np.abs(train_rescref.T[j] - train_rescpred.T[j])))  \n",
    "    \n",
    "    # Mean Square Error \n",
    "    seg_msel.append(np.mean(np.square(train_rescref.T[j] - train_rescpred.T[j]))) \n",
    "       \n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_mape.append(mean_absolute_percentage_error(train_rescref.T[j],train_rescpred.T[j]))\n",
    "    \n",
    "    # Accuracy\n",
    "    seg_acc.append(1-np.linalg.norm(train_rescref.T[j]-train_rescpred.T[j])/np.linalg.norm(train_rescref.T[j]))\n",
    "    \n",
    "    # Coefficient of determination\n",
    "    seg_R2.append(r2_score(train_rescref.T[j],train_rescpred.T[j]))\n",
    "    \n",
    "    # Explained variance score\n",
    "    seg_var.append(explained_variance_score(train_rescref.T[j],train_rescpred.T[j]))\n",
    "    \n",
    "    if seg_nmael[-1] != 0:\n",
    "        seg_masel.append(\n",
    "            seg_mael[-1] / seg_nmael[-1]\n",
    "        )  # Ratio of the two: Mean Absolute Scaled Error\n",
    "    else:\n",
    "        seg_masel.append(np.NaN)\n",
    "\n",
    "print(\"Total (ave) MAE for NN: \" + str(np.mean(np.array(seg_mael))))\n",
    "print(\"Total (ave) RMSE for NN: \" + str(np.mean(np.sqrt(np.array(seg_msel)))))\n",
    "print(\"Total (ave) MAPE for NN: \" + str(np.mean(np.array(seg_mape))))\n",
    "print(\"Total (ave) Accuracy for NN: \" + str(np.mean(np.array(seg_acc))))\n",
    "print(\"Total (ave) R2 for NN: \" + str(np.mean(np.array(seg_R2))))\n",
    "print(\"Total (ave) Var for NN: \" + str(np.mean(np.array(seg_var))))\n",
    "print(\"Total (ave) MASE for per-segment NN/naive MAE: \" + str(np.nanmean(np.array(seg_masel))))\n",
    "print(\"...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction.\")\n",
    "\n",
    "print(\"\\nTotal (ave) MAE for naive prediction: \" + str(np.mean(np.array(seg_nmael))))\n",
    "print(\"Total (ave) RMSE for naive prediction: \" + str(np.mean(np.sqrt(np.array(seg_nmsel)))))\n",
    "print(\"Total (ave) MAPE for naive prediction: \" + str(np.mean(np.array(seg_nmape))))\n",
    "print(\"Total (ave) Accuracy for naive prediction: \" + str(np.mean(np.array(seg_nacc))))\n",
    "print(\"Total (ave) R2 for naive prediction: \" + str(np.mean(np.array(seg_nR2))))\n",
    "print(\"Total (ave) Var for naive prediction: \" + str(np.mean(np.array(seg_nvar))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "seg_nmael = [] #naïve mae\n",
    "seg_nmsel = [] #naïve mse\n",
    "seg_nmape = [] #naïve mape\n",
    "seg_nacc = []  #naïve accuracy\n",
    "seg_nR2 = []   #naïve coeff of determination\n",
    "seg_nvar = []  #naïve coeff of variance score\n",
    "\n",
    "seg_mael = []  #mae\n",
    "seg_masel = [] #mase\n",
    "seg_msel = []  #mse\n",
    "seg_mape = []  #mape\n",
    "seg_acc = []   #accuracy\n",
    "seg_R2 = []    #coeff of determination\n",
    "seg_var = []   #coeff of variance score\n",
    "\n",
    "kk=num_nodes\n",
    "for j in range(kk):\n",
    "\n",
    "    ## NAIVE\n",
    "    # Mean Square Error\n",
    "    seg_nmsel.append(np.mean(np.square(valid_rescref.T[j] - validnpredc.T[j])))\n",
    "\n",
    "    # Mean Absolute Error\n",
    "    seg_nmael.append(np.mean(np.abs(valid_rescref.T[j] - validnpredc.T[j])))\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_nmape.append(mean_absolute_percentage_error(valid_rescref.T[j], validnpredc.T[j]))\n",
    "\n",
    "    # Accuracy\n",
    "    seg_nacc.append(1 - np.linalg.norm(valid_rescref.T[j] - validnpredc.T[j]) / np.linalg.norm(valid_rescref.T[j]))\n",
    "\n",
    "    # Coefficient of determination\n",
    "    seg_nR2.append(r2_score(valid_rescref.T[j], validnpredc.T[j]))\n",
    "\n",
    "    # Explained variance score\n",
    "    seg_nvar.append(explained_variance_score(valid_rescref.T[j], validnpredc.T[j]))\n",
    "\n",
    "    ## NN\n",
    "    # Mean Absolute Error\n",
    "    seg_mael.append(np.mean(np.abs(valid_rescref.T[j] - valid_rescpred.T[j])))\n",
    "\n",
    "    # Mean Square Error\n",
    "    seg_msel.append(np.mean(np.square(valid_rescref.T[j] - valid_rescpred.T[j])))\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_mape.append(mean_absolute_percentage_error(valid_rescref.T[j], valid_rescpred.T[j]))\n",
    "\n",
    "    # Accuracy\n",
    "    seg_acc.append(1 - np.linalg.norm(valid_rescref.T[j] - valid_rescpred.T[j]) / np.linalg.norm(valid_rescref.T[j]))\n",
    "\n",
    "    # Coefficient of determination\n",
    "    seg_R2.append(r2_score(valid_rescref.T[j], valid_rescpred.T[j]))\n",
    "\n",
    "    # Explained variance score\n",
    "    seg_var.append(explained_variance_score(valid_rescref.T[j], valid_rescpred.T[j]))\n",
    "\n",
    "    if seg_nmael[-1] != 0:\n",
    "        seg_masel.append(seg_mael[-1] / seg_nmael[-1])  # Ratio of the two: Mean Absolute Scaled Error\n",
    "    else:\n",
    "        seg_masel.append(np.NaN)\n",
    "\n",
    "print(\"Total (ave) MAE for NN: \" + str(np.mean(np.array(seg_mael))))\n",
    "print(\"Total (ave) RMSE for NN: \" + str(np.mean(np.sqrt(np.array(seg_msel)))))\n",
    "print(\"Total (ave) MAPE for NN: \" + str(np.mean(np.array(seg_mape))))\n",
    "print(\"Total (ave) Accuracy for NN: \" + str(np.mean(np.array(seg_acc))))\n",
    "print(\"Total (ave) R2 for NN: \" + str(np.mean(np.array(seg_R2))))\n",
    "print(\"Total (ave) Var for NN: \" + str(np.mean(np.array(seg_var))))\n",
    "print(\"Total (ave) MASE for per-segment NN/naive MAE: \" + str(np.nanmean(np.array(seg_masel))))\n",
    "print(\"...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction.\")\n",
    "\n",
    "print(\"\\nTotal (ave) MAE for naive prediction: \" + str(np.mean(np.array(seg_nmael))))\n",
    "print(\"Total (ave) RMSE for naive prediction: \" + str(np.mean(np.sqrt(np.array(seg_nmsel)))))\n",
    "print(\"Total (ave) MAPE for naive prediction: \" + str(np.mean(np.array(seg_nmape))))\n",
    "print(\"Total (ave) Accuracy for naive prediction: \" + str(np.mean(np.array(seg_nacc))))\n",
    "print(\"Total (ave) R2 for naive prediction: \" + str(np.mean(np.array(seg_nR2))))\n",
    "print(\"Total (ave) Var for naive prediction: \" + str(np.mean(np.array(seg_nvar))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "seg_nmael = [] #naïve mae\n",
    "seg_nmsel = [] #naïve mse\n",
    "seg_nmape = [] #naïve mape\n",
    "seg_nacc = []  #naïve accuracy\n",
    "seg_nR2 = []   #naïve coeff of determination\n",
    "seg_nvar = []  #naïve coeff of variance score\n",
    "\n",
    "seg_mael = []  #mae\n",
    "seg_masel = [] #mase\n",
    "seg_msel = []  #mse\n",
    "seg_mape = []  #mape\n",
    "seg_acc = []   #accuracy\n",
    "seg_R2 = []    #coeff of determination\n",
    "seg_var = []   #coeff of variance score\n",
    "\n",
    "kk=num_nodes\n",
    "for j in range(kk):\n",
    "\n",
    "    ## NAIVE\n",
    "    # Mean Square Error\n",
    "    seg_nmsel.append(np.mean(np.square(test_rescref.T[j] - testnpredc.T[j])))\n",
    "\n",
    "    # Mean Absolute Error\n",
    "    seg_nmael.append(np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j])))\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_nmape.append(mean_absolute_percentage_error(test_rescref.T[j], testnpredc.T[j]))\n",
    "\n",
    "    # Accuracy\n",
    "    seg_nacc.append(1 - np.linalg.norm(test_rescref.T[j] - testnpredc.T[j]) / np.linalg.norm(test_rescref.T[j]))\n",
    "\n",
    "    # Coefficient of determination\n",
    "    seg_nR2.append(r2_score(test_rescref.T[j], testnpredc.T[j]))\n",
    "\n",
    "    # Explained variance score\n",
    "    seg_nvar.append(explained_variance_score(test_rescref.T[j], testnpredc.T[j]))\n",
    "\n",
    "    ## NN\n",
    "    # Mean Absolute Error\n",
    "    seg_mael.append(np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j])))\n",
    "\n",
    "    # Mean Square Error\n",
    "    seg_msel.append(np.mean(np.square(test_rescref.T[j] - test_rescpred.T[j])))\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_mape.append(mean_absolute_percentage_error(test_rescref.T[j], test_rescpred.T[j]))\n",
    "\n",
    "    # Accuracy\n",
    "    seg_acc.append(1 - np.linalg.norm(test_rescref.T[j] - test_rescpred.T[j]) / np.linalg.norm(test_rescref.T[j]))\n",
    "\n",
    "    # Coefficient of determination\n",
    "    seg_R2.append(r2_score(test_rescref.T[j], test_rescpred.T[j]))\n",
    "\n",
    "    # Explained variance score\n",
    "    seg_var.append(explained_variance_score(test_rescref.T[j], test_rescpred.T[j]))\n",
    "\n",
    "    if seg_nmael[-1] != 0:\n",
    "        seg_masel.append(seg_mael[-1] / seg_nmael[-1])  # Ratio of the two: Mean Absolute Scaled Error\n",
    "    else:\n",
    "        seg_masel.append(np.NaN)\n",
    "\n",
    "print(\"Total (ave) MAE for NN: \" + str(np.mean(np.array(seg_mael))))\n",
    "print(\"Total (ave) RMSE for NN: \" + str(np.mean(np.sqrt(np.array(seg_msel)))))\n",
    "print(\"Total (ave) MAPE for NN: \" + str(np.mean(np.array(seg_mape))))\n",
    "print(\"Total (ave) Accuracy for NN: \" + str(np.mean(np.array(seg_acc))))\n",
    "print(\"Total (ave) R2 for NN: \" + str(np.mean(np.array(seg_R2))))\n",
    "print(\"Total (ave) Var for NN: \" + str(np.mean(np.array(seg_var))))\n",
    "print(\"Total (ave) MASE for per-segment NN/naive MAE: \" + str(np.nanmean(np.array(seg_masel))))\n",
    "print(\"...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction.\")\n",
    "\n",
    "print(\"\\nTotal (ave) MAE for naive prediction: \" + str(np.mean(np.array(seg_nmael))))\n",
    "print(\"Total (ave) RMSE for naive prediction: \" + str(np.mean(np.sqrt(np.array(seg_nmsel)))))\n",
    "print(\"Total (ave) MAPE for naive prediction: \" + str(np.mean(np.array(seg_nmape))))\n",
    "print(\"Total (ave) Accuracy for naive prediction: \" + str(np.mean(np.array(seg_nacc))))\n",
    "print(\"Total (ave) R2 for naive prediction: \" + str(np.mean(np.array(seg_nR2))))\n",
    "print(\"Total (ave) Var for naive prediction: \" + str(np.mean(np.array(seg_nvar))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'metrics/MFSTC-CNN-LSTM_24_1_32_16/'    #seq_len _ pre_len _ batch_size _ hidden_units\n",
    "\n",
    "a_file = open(mypath+\"MAE.txt\", \"w\")\n",
    "for row in np.matrix(seg_mael):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"RMSE.txt\", \"w\")\n",
    "for row in np.matrix(seg_msel):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"MASE.txt\", \"w\")\n",
    "for row in np.matrix(seg_masel):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"ACC.txt\", \"w\")\n",
    "for row in np.matrix(seg_acc):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"R2.txt\", \"w\")\n",
    "for row in np.matrix(seg_R2):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"EVS.txt\", \"w\")\n",
    "for row in np.matrix(seg_var):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Result visualization\n",
    "fig1 = plt.figure(figsize=(15, 8))\n",
    "#    ax1 = fig1.add_subplot(1,1,1)\n",
    "a_pred = test_rescpred[0:, 20]\n",
    "a_true = test_rescref[0:, 20]\n",
    "plt.plot(a_true, \"tab:green\", label=\"true\",linewidth=0.5)\n",
    "plt.plot(a_pred, \"tab:blue\", label=\"prediction\",linewidth=0.5)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"speed\")\n",
    "plt.legend(loc=\"best\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'models_MFSTC-CNN-LSTM/24_1_32_16/'    #seq_len _ pre_len _ batch_size _ hidden_units\n",
    "model.save(mypath + 'model.h5')\n",
    "\n",
    "a_file = open(mypath+\"train_rescref.txt\", \"w\")\n",
    "for row in np.matrix(train_rescref):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"train_rescpred.txt\", \"w\")\n",
    "for row in np.matrix(train_rescpred):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"valid_rescref.txt\", \"w\")\n",
    "for row in np.matrix(valid_rescref):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"valid_rescpred.txt\", \"w\")\n",
    "for row in np.matrix(valid_rescpred):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"test_rescref.txt\", \"w\")\n",
    "for row in np.matrix(test_rescref):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"test_rescpred.txt\", \"w\")\n",
    "for row in np.matrix(test_rescpred):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
