{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucía García-Duarte Sáenz\n",
    "# Stacked-LSTM Model\n",
    "\n",
    "- Architecture: \n",
    "  1. User defined number of LSTM layers. \n",
    "  3. A Dropout and a Dense layer as they experimentally showed improvement in performance and managing over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "- An NxT feature matrix, which describes the ($T_1, \\cdots, T_t$) temperature records over t timesteps for the N stations.\n",
    "\n",
    "Temperature is recorded every hour over 37 stations for 26 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mat = pd.read_csv('feat_matrix.csv')\n",
    "feat_mat = feat_mat.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "num_nodes, time_len = feat_mat.shape\n",
    "print(\"No. of stations:\", num_nodes, \"\\nNo. of timesteps:\", time_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "\n",
    "Define functions to split the data into training, validation and test, to normalize the data, and to prepare the data to be fed into the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data, valid_portion, test_portion):\n",
    "    \n",
    "    time_len = data.shape[1]\n",
    "    train_portion = 1 - valid_portion - test_portion\n",
    "    train_size = int(time_len * train_portion)\n",
    "    valid_size = int(time_len * valid_portion)\n",
    "    \n",
    "    train_data = np.array(data.iloc[:, :train_size])\n",
    "    valid_data = np.array(data.iloc[:, train_size:train_size+valid_size])\n",
    "    test_data = np.array(data.iloc[:, train_size+valid_size:])\n",
    "    \n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "\n",
    "def scale_data(train_data, valid_data, test_data):\n",
    "    \n",
    "    max_speed = train_data.max()\n",
    "    min_speed = train_data.min()\n",
    "    \n",
    "    train_scaled = (train_data - min_speed) / (max_speed - min_speed)\n",
    "    valid_scaled = (valid_data - min_speed) / (max_speed - min_speed)\n",
    "    test_scaled = (test_data - min_speed) / (max_speed - min_speed)\n",
    "    \n",
    "    return train_scaled, valid_scaled, test_scaled\n",
    "\n",
    "\n",
    "def sequence_data_preparation(seq_len, pre_len, train_data, valid_data, test_data):\n",
    "    \n",
    "    trainX, trainY, validX, validY, testX, testY = [], [], [], [], [], []\n",
    "\n",
    "    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        a = train_data[:, i : i + seq_len + pre_len]\n",
    "        trainX.append(a[:, :seq_len])\n",
    "        trainY.append(a[:, -1])\n",
    "        \n",
    "    for i in range(valid_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        b = valid_data[:, i : i + seq_len + pre_len]\n",
    "        validX.append(b[:, :seq_len])\n",
    "        validY.append(b[:, -1])\n",
    "        \n",
    "    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        c = test_data[:, i : i + seq_len + pre_len]\n",
    "        testX.append(c[:, :seq_len])\n",
    "        testY.append(c[:, -1])\n",
    "\n",
    "    trainX = np.array(trainX)\n",
    "    trainY = np.array(trainY)\n",
    "    validX = np.array(validX)\n",
    "    validY = np.array(validY)\n",
    "    testX = np.array(testX)\n",
    "    testY = np.array(testY)\n",
    "\n",
    "    return trainX, trainY, validX, validY, testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rate = 0.3\n",
    "valid_rate = 0.1\n",
    "train_data, valid_data, test_data = train_val_test_split(feat_mat, valid_rate, test_rate)\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Valid data: \", valid_data.shape)\n",
    "print(\"Test data: \", test_data.shape)\n",
    "feat_mat.shape[1] - train_data.shape[1] - valid_data.shape[1] - test_data.shape[1] # check dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled, valid_scaled, test_scaled = scale_data(train_data, valid_data, test_data)\n",
    "#del feat_mat # to save memory for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 24    # change by 24, 24*7, 24*7*2, 24*7*3 (h)\n",
    "pre_len = 1     # change by 1, 2, 3 (h)\n",
    "\n",
    "trainX, trainY, validX, validY, testX, testY = sequence_data_preparation(\n",
    "    seq_len, pre_len, train_scaled, valid_scaled, test_scaled\n",
    ")\n",
    "\n",
    "trainX = trainX.reshape((trainX.shape[0], trainX.shape[2], -1)) \n",
    "trainY = trainY.reshape((trainY.shape[0], -1)) \n",
    "validX = validX.reshape((validX.shape[0], validX.shape[2], -1)) \n",
    "validY = validY.reshape((validY.shape[0], -1)) \n",
    "testX = testX.reshape((testX.shape[0], testX.shape[2], -1)) \n",
    "testY = testY.reshape((testY.shape[0], -1)) \n",
    "\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(validX.shape)\n",
    "print(validY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session() # clear previous model\n",
    "model = Sequential()                                                     \n",
    "model.add(LSTM(16, activation='tanh', return_sequences=True, input_shape=(seq_len,num_nodes))) # change by 16, 32, 64, 128, 256\n",
    "model.add(LSTM(16, activation='tanh'))                                                         # change by 16, 32, 64, 128, 256\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_nodes, activation=\"sigmoid\"))\n",
    "model.compile(optimizer='adam', loss='mae',metrics=[\"mse\"])\n",
    "\n",
    "# Callbacks\n",
    "#from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "#mc = ModelCheckpoint('models_Stacked-LSTM/24_1_32_16/best_model.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_start = datetime.datetime.now()\n",
    "history = model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    #verbose=0,\n",
    "    validation_data=(validX, validY)#, callbacks = [mc]\n",
    ")\n",
    "train_end = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Train loss: \",\n",
    "    history.history[\"loss\"][-1],\n",
    "    \"\\nValid loss:\",\n",
    "    history.history[\"val_loss\"][-1],\n",
    "    \"\\nElapsed time: \",\n",
    "    train_end-train_start,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['mse'])\n",
    "plt.plot(history.history['val_mse'])\n",
    "plt.ylabel('mse')\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##If loading best model\n",
    "#from tensorflow import keras\n",
    "#model=keras.models.load_model(\"models_Stacked-LSTM/24_1_32_16/best_model.h5\")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ythat = model.predict(trainX)\n",
    "yhat = model.predict(testX)\n",
    "yh = model.predict(validX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rescale values\n",
    "max_speed = train_data.max()\n",
    "min_speed = train_data.min()\n",
    "\n",
    "## actual train and test values\n",
    "train_rescref = np.array(trainY * max_speed)\n",
    "test_rescref = np.array(testY * max_speed)\n",
    "valid_rescref = np.array(validY * max_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rescpred = np.array((ythat) * max_speed)\n",
    "test_rescpred = np.array((yhat) * max_speed)\n",
    "valid_rescpred = np.array((yh) * max_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance measures\n",
    "from sklearn.metrics import r2_score, explained_variance_score#, mean_absolute_percentage_error\n",
    "from sklearn.metrics.regression import _check_reg_targets, check_consistent_length\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred,\n",
    "                                   sample_weight=None,\n",
    "                                   multioutput='uniform_average'):\n",
    "\n",
    "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
    "        y_true, y_pred, multioutput)\n",
    "    check_consistent_length(y_true, y_pred, sample_weight)\n",
    "    epsilon = np.finfo(np.float64).eps\n",
    "    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\n",
    "    output_errors = np.average(mape,\n",
    "                               weights=sample_weight, axis=0)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            return output_errors\n",
    "        elif multioutput == 'uniform_average':\n",
    "            # pass None as weights to np.average: uniform mean\n",
    "            multioutput = None\n",
    "\n",
    "    return np.average(output_errors, weights=multioutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naïve\n",
    "trainX=trainX.reshape((trainX.shape[0], trainX.shape[2], -1)) \n",
    "trainnpred = np.array(trainX)[\n",
    "    :, :, -1\n",
    "]  \n",
    "trainnpredc = (trainnpred) * max_speed\n",
    "\n",
    "# Metrics\n",
    "seg_nmael = [] #naïve mae\n",
    "seg_nmsel = [] #naïve mse\n",
    "seg_nmape = [] #naïve mape\n",
    "seg_nacc = []  #naïve accuracy\n",
    "seg_nR2 = []   #naïve coeff of determination\n",
    "seg_nvar = []  #naïve coeff of variance score\n",
    "\n",
    "seg_mael = []  #mae\n",
    "seg_masel = [] #mase\n",
    "seg_msel = []  #mse\n",
    "seg_mape = []  #mape\n",
    "seg_acc = []   #accuracy\n",
    "seg_R2 = []    #coeff of determination\n",
    "seg_var = []   #coeff of variance score\n",
    "\n",
    "kk=trainX.shape[1]\n",
    "for j in range(kk):\n",
    "    \n",
    "    ## NAIVE\n",
    "    # Mean Square Error \n",
    "    seg_nmsel.append(np.mean(np.square(train_rescref.T[j] - trainnpredc.T[j]))) \n",
    "        \n",
    "    # Mean Absolute Error \n",
    "    seg_nmael.append(np.mean(np.abs(train_rescref.T[j] - trainnpredc.T[j])))  \n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_nmape.append(mean_absolute_percentage_error(train_rescref.T[j],trainnpredc.T[j]))\n",
    "    \n",
    "    # Accuracy\n",
    "    seg_nacc.append(1-np.linalg.norm(train_rescref.T[j]-trainnpredc.T[j])/np.linalg.norm(train_rescref.T[j]))\n",
    "    \n",
    "    # Coefficient of determination\n",
    "    seg_nR2.append(r2_score(train_rescref.T[j],trainnpredc.T[j]))\n",
    "    \n",
    "    # Explained variance score\n",
    "    seg_nvar.append(explained_variance_score(train_rescref.T[j],trainnpredc.T[j]))\n",
    "    \n",
    "    \n",
    "    ## NN\n",
    "    # Mean Absolute Error \n",
    "    seg_mael.append(np.mean(np.abs(train_rescref.T[j] - train_rescpred.T[j])))  \n",
    "    \n",
    "    # Mean Square Error \n",
    "    seg_msel.append(np.mean(np.square(train_rescref.T[j] - train_rescpred.T[j]))) \n",
    "       \n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_mape.append(mean_absolute_percentage_error(train_rescref.T[j],train_rescpred.T[j]))\n",
    "    \n",
    "    # Accuracy\n",
    "    seg_acc.append(1-np.linalg.norm(train_rescref.T[j]-train_rescpred.T[j])/np.linalg.norm(train_rescref.T[j]))\n",
    "    \n",
    "    # Coefficient of determination\n",
    "    seg_R2.append(r2_score(train_rescref.T[j],train_rescpred.T[j]))\n",
    "    \n",
    "    # Explained variance score\n",
    "    seg_var.append(explained_variance_score(train_rescref.T[j],train_rescpred.T[j]))\n",
    "    \n",
    "    if seg_nmael[-1] != 0:\n",
    "        seg_masel.append(\n",
    "            seg_mael[-1] / seg_nmael[-1]\n",
    "        )  # Ratio of the two: Mean Absolute Scaled Error\n",
    "    else:\n",
    "        seg_masel.append(np.NaN)\n",
    "\n",
    "print(\"Total (ave) MAE for NN: \" + str(np.mean(np.array(seg_mael))))\n",
    "print(\"Total (ave) RMSE for NN: \" + str(np.mean(np.sqrt(np.array(seg_msel)))))\n",
    "print(\"Total (ave) MAPE for NN: \" + str(np.mean(np.array(seg_mape))))\n",
    "print(\"Total (ave) Accuracy for NN: \" + str(np.mean(np.array(seg_acc))))\n",
    "print(\"Total (ave) R2 for NN: \" + str(np.mean(np.array(seg_R2))))\n",
    "print(\"Total (ave) Var for NN: \" + str(np.mean(np.array(seg_var))))\n",
    "print(\"Total (ave) MASE for per-segment NN/naive MAE: \" + str(np.nanmean(np.array(seg_masel))))\n",
    "print(\"...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction.\")\n",
    "\n",
    "print(\"\\nTotal (ave) MAE for naive prediction: \" + str(np.mean(np.array(seg_nmael))))\n",
    "print(\"Total (ave) RMSE for naive prediction: \" + str(np.mean(np.sqrt(np.array(seg_nmsel)))))\n",
    "print(\"Total (ave) MAPE for naive prediction: \" + str(np.mean(np.array(seg_nmape))))\n",
    "print(\"Total (ave) Accuracy for naive prediction: \" + str(np.mean(np.array(seg_nacc))))\n",
    "print(\"Total (ave) R2 for naive prediction: \" + str(np.mean(np.array(seg_nR2))))\n",
    "print(\"Total (ave) Var for naive prediction: \" + str(np.mean(np.array(seg_nvar))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naïve\n",
    "validX=validX.reshape((validX.shape[0], validX.shape[2], -1)) \n",
    "validnpred = np.array(validX)[\n",
    "    :, :, -1\n",
    "] \n",
    "validnpredc = (validnpred) * max_speed\n",
    "\n",
    "# Metrics\n",
    "seg_nmael = [] #naïve mae\n",
    "seg_nmsel = [] #naïve mse\n",
    "seg_nmape = [] #naïve mape\n",
    "seg_nacc = []  #naïve accuracy\n",
    "seg_nR2 = []   #naïve coeff of determination\n",
    "seg_nvar = []  #naïve coeff of variance score\n",
    "\n",
    "seg_mael = []  #mae\n",
    "seg_masel = [] #mase\n",
    "seg_msel = []  #mse\n",
    "seg_mape = []  #mape\n",
    "seg_acc = []   #accuracy\n",
    "seg_R2 = []    #coeff of determination\n",
    "seg_var = []   #coeff of variance score\n",
    "\n",
    "kk=validX.shape[1]\n",
    "for j in range(kk):\n",
    "\n",
    "    ## NAIVE\n",
    "    # Mean Square Error\n",
    "    seg_nmsel.append(np.mean(np.square(valid_rescref.T[j] - validnpredc.T[j])))\n",
    "\n",
    "    # Mean Absolute Error\n",
    "    seg_nmael.append(np.mean(np.abs(valid_rescref.T[j] - validnpredc.T[j])))\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_nmape.append(mean_absolute_percentage_error(valid_rescref.T[j], validnpredc.T[j]))\n",
    "\n",
    "    # Accuracy\n",
    "    seg_nacc.append(1 - np.linalg.norm(valid_rescref.T[j] - validnpredc.T[j]) / np.linalg.norm(valid_rescref.T[j]))\n",
    "\n",
    "    # Coefficient of determination\n",
    "    seg_nR2.append(r2_score(valid_rescref.T[j], validnpredc.T[j]))\n",
    "\n",
    "    # Explained variance score\n",
    "    seg_nvar.append(explained_variance_score(valid_rescref.T[j], validnpredc.T[j]))\n",
    "\n",
    "    ## NN\n",
    "    # Mean Absolute Error\n",
    "    seg_mael.append(np.mean(np.abs(valid_rescref.T[j] - valid_rescpred.T[j])))\n",
    "\n",
    "    # Mean Square Error\n",
    "    seg_msel.append(np.mean(np.square(valid_rescref.T[j] - valid_rescpred.T[j])))\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_mape.append(mean_absolute_percentage_error(valid_rescref.T[j], valid_rescpred.T[j]))\n",
    "\n",
    "    # Accuracy\n",
    "    seg_acc.append(1 - np.linalg.norm(valid_rescref.T[j] - valid_rescpred.T[j]) / np.linalg.norm(valid_rescref.T[j]))\n",
    "\n",
    "    # Coefficient of determination\n",
    "    seg_R2.append(r2_score(valid_rescref.T[j], valid_rescpred.T[j]))\n",
    "\n",
    "    # Explained variance score\n",
    "    seg_var.append(explained_variance_score(valid_rescref.T[j], valid_rescpred.T[j]))\n",
    "\n",
    "    if seg_nmael[-1] != 0:\n",
    "        seg_masel.append(seg_mael[-1] / seg_nmael[-1])  # Ratio of the two: Mean Absolute Scaled Error\n",
    "    else:\n",
    "        seg_masel.append(np.NaN)\n",
    "\n",
    "print(\"Total (ave) MAE for NN: \" + str(np.mean(np.array(seg_mael))))\n",
    "print(\"Total (ave) RMSE for NN: \" + str(np.mean(np.sqrt(np.array(seg_msel)))))\n",
    "print(\"Total (ave) MAPE for NN: \" + str(np.mean(np.array(seg_mape))))\n",
    "print(\"Total (ave) Accuracy for NN: \" + str(np.mean(np.array(seg_acc))))\n",
    "print(\"Total (ave) R2 for NN: \" + str(np.mean(np.array(seg_R2))))\n",
    "print(\"Total (ave) Var for NN: \" + str(np.mean(np.array(seg_var))))\n",
    "print(\"Total (ave) MASE for per-segment NN/naive MAE: \" + str(np.nanmean(np.array(seg_masel))))\n",
    "print(\"...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction.\")\n",
    "\n",
    "print(\"\\nTotal (ave) MAE for naive prediction: \" + str(np.mean(np.array(seg_nmael))))\n",
    "print(\"Total (ave) RMSE for naive prediction: \" + str(np.mean(np.sqrt(np.array(seg_nmsel)))))\n",
    "print(\"Total (ave) MAPE for naive prediction: \" + str(np.mean(np.array(seg_nmape))))\n",
    "print(\"Total (ave) Accuracy for naive prediction: \" + str(np.mean(np.array(seg_nacc))))\n",
    "print(\"Total (ave) R2 for naive prediction: \" + str(np.mean(np.array(seg_nR2))))\n",
    "print(\"Total (ave) Var for naive prediction: \" + str(np.mean(np.array(seg_nvar))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naïve\n",
    "testX=testX.reshape((testX.shape[0], testX.shape[2], -1)) \n",
    "testnpred = np.array(testX)[\n",
    "    :, :, -1\n",
    "]  \n",
    "testnpredc = (testnpred) * max_speed\n",
    "\n",
    "# Metrics\n",
    "seg_nmael = [] #naïve mae\n",
    "seg_nmsel = [] #naïve mse\n",
    "seg_nmape = [] #naïve mape\n",
    "seg_nacc = []  #naïve accuracy\n",
    "seg_nR2 = []   #naïve coeff of determination\n",
    "seg_nvar = []  #naïve coeff of variance score\n",
    "\n",
    "seg_mael = []  #mae\n",
    "seg_masel = [] #mase\n",
    "seg_msel = []  #mse\n",
    "seg_mape = []  #mape\n",
    "seg_acc = []   #accuracy\n",
    "seg_R2 = []    #coeff of determination\n",
    "seg_var = []   #coeff of variance score\n",
    "\n",
    "kk=testX.shape[1]\n",
    "for j in range(kk):\n",
    "\n",
    "    ## NAIVE\n",
    "    # Mean Square Error\n",
    "    seg_nmsel.append(np.mean(np.square(test_rescref.T[j] - testnpredc.T[j])))\n",
    "\n",
    "    # Mean Absolute Error\n",
    "    seg_nmael.append(np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j])))\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_nmape.append(mean_absolute_percentage_error(test_rescref.T[j], testnpredc.T[j]))\n",
    "\n",
    "    # Accuracy\n",
    "    seg_nacc.append(1 - np.linalg.norm(test_rescref.T[j] - testnpredc.T[j]) / np.linalg.norm(test_rescref.T[j]))\n",
    "\n",
    "    # Coefficient of determination\n",
    "    seg_nR2.append(r2_score(test_rescref.T[j], testnpredc.T[j]))\n",
    "\n",
    "    # Explained variance score\n",
    "    seg_nvar.append(explained_variance_score(test_rescref.T[j], testnpredc.T[j]))\n",
    "\n",
    "    ## NN\n",
    "    # Mean Absolute Error\n",
    "    seg_mael.append(np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j])))\n",
    "\n",
    "    # Mean Square Error\n",
    "    seg_msel.append(np.mean(np.square(test_rescref.T[j] - test_rescpred.T[j])))\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_mape.append(mean_absolute_percentage_error(test_rescref.T[j], test_rescpred.T[j]))\n",
    "\n",
    "    # Accuracy\n",
    "    seg_acc.append(1 - np.linalg.norm(test_rescref.T[j] - test_rescpred.T[j]) / np.linalg.norm(test_rescref.T[j]))\n",
    "\n",
    "    # Coefficient of determination\n",
    "    seg_R2.append(r2_score(test_rescref.T[j], test_rescpred.T[j]))\n",
    "\n",
    "    # Explained variance score\n",
    "    seg_var.append(explained_variance_score(test_rescref.T[j], test_rescpred.T[j]))\n",
    "\n",
    "    if seg_nmael[-1] != 0:\n",
    "        seg_masel.append(seg_mael[-1] / seg_nmael[-1])  # Ratio of the two: Mean Absolute Scaled Error\n",
    "    else:\n",
    "        seg_masel.append(np.NaN)\n",
    "\n",
    "print(\"Total (ave) MAE for NN: \" + str(np.mean(np.array(seg_mael))))\n",
    "print(\"Total (ave) RMSE for NN: \" + str(np.mean(np.sqrt(np.array(seg_msel)))))\n",
    "print(\"Total (ave) MAPE for NN: \" + str(np.mean(np.array(seg_mape))))\n",
    "print(\"Total (ave) Accuracy for NN: \" + str(np.mean(np.array(seg_acc))))\n",
    "print(\"Total (ave) R2 for NN: \" + str(np.mean(np.array(seg_R2))))\n",
    "print(\"Total (ave) Var for NN: \" + str(np.mean(np.array(seg_var))))\n",
    "print(\"Total (ave) MASE for per-segment NN/naive MAE: \" + str(np.nanmean(np.array(seg_masel))))\n",
    "print(\"...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction.\")\n",
    "\n",
    "print(\"\\nTotal (ave) MAE for naive prediction: \" + str(np.mean(np.array(seg_nmael))))\n",
    "print(\"Total (ave) RMSE for naive prediction: \" + str(np.mean(np.sqrt(np.array(seg_nmsel)))))\n",
    "print(\"Total (ave) MAPE for naive prediction: \" + str(np.mean(np.array(seg_nmape))))\n",
    "print(\"Total (ave) Accuracy for naive prediction: \" + str(np.mean(np.array(seg_nacc))))\n",
    "print(\"Total (ave) R2 for naive prediction: \" + str(np.mean(np.array(seg_nR2))))\n",
    "print(\"Total (ave) Var for naive prediction: \" + str(np.mean(np.array(seg_nvar))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'metrics/Stacked-LSTM_24_1_32_16/'    #seq_len _ pre_len _ batch_size _ hidden_units\n",
    "\n",
    "a_file = open(mypath+\"MAE.txt\", \"w\")\n",
    "for row in np.matrix(seg_mael):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"RMSE.txt\", \"w\")\n",
    "for row in np.matrix(seg_msel):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"MASE.txt\", \"w\")\n",
    "for row in np.matrix(seg_masel):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"ACC.txt\", \"w\")\n",
    "for row in np.matrix(seg_acc):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"R2.txt\", \"w\")\n",
    "for row in np.matrix(seg_R2):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"EVS.txt\", \"w\")\n",
    "for row in np.matrix(seg_var):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Result visualization\n",
    "fig1 = plt.figure(figsize=(15, 8))\n",
    "#    ax1 = fig1.add_subplot(1,1,1)\n",
    "a_pred = test_rescpred[0:, 20]\n",
    "a_true = test_rescref[0:, 20]\n",
    "plt.plot(a_true, \"tab:green\", label=\"true\",linewidth=0.5)\n",
    "plt.plot(a_pred, \"tab:blue\", label=\"prediction\",linewidth=0.5)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"speed\")\n",
    "plt.legend(loc=\"best\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'models_Stacked-LSTM/24_1_32_16/'      #seq_len _ pre_len _ batch_size _ hidden_units\n",
    "model.save(mypath + 'model.h5')\n",
    "\n",
    "a_file = open(mypath+\"train_rescref.txt\", \"w\")\n",
    "for row in np.matrix(train_rescref):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"train_rescpred.txt\", \"w\")\n",
    "for row in np.matrix(train_rescpred):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"valid_rescref.txt\", \"w\")\n",
    "for row in np.matrix(valid_rescref):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"valid_rescpred.txt\", \"w\")\n",
    "for row in np.matrix(valid_rescpred):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"test_rescref.txt\", \"w\")\n",
    "for row in np.matrix(test_rescref):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"test_rescpred.txt\", \"w\")\n",
    "for row in np.matrix(test_rescpred):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
