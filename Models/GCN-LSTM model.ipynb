{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjm4FO92qgPX"
   },
   "source": [
    "Lucía García-Duarte Sáenz\n",
    "# GCN-LSTM Model\n",
    "\n",
    "- Architecture: \n",
    "  1. User defined number of graph convolutional layers (Reference: Kipf & Welling (ICLR 2017)).\n",
    "  2. User defined number of LSTM layers. The TGCN uses GRU instead of LSTM. In practice there are not any remarkable differences between the two types of layers. We use LSTM as they are more frequently used.\n",
    "  3. A Dropout and a Dense layer as they experimentally showed improvement in performance and managing over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16030,
     "status": "ok",
     "timestamp": 1619819072688,
     "user": {
      "displayName": "LUCIA GARCIA - DUARTE SAENZ",
      "photoUrl": "",
      "userId": "14289334807482311885"
     },
     "user_tz": -120
    },
    "id": "4d0g2GEDqlnz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.layer import GCN_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Quv95zlsv9E5"
   },
   "source": [
    "## Load the data\n",
    "- An NxN adjacency matrix, which describes the distance relationship between the N stations,\n",
    "- An NxT feature matrix, which describes the ($T_1, \\cdots, T_t$) temperature records over t timesteps for the N stations.\n",
    "\n",
    "Temperature is recorded every hour over 37 stations for 26 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16050,
     "status": "ok",
     "timestamp": 1619819073345,
     "user": {
      "displayName": "LUCIA GARCIA - DUARTE SAENZ",
      "photoUrl": "",
      "userId": "14289334807482311885"
     },
     "user_tz": -120
    },
    "id": "eOjS066-EHIn"
   },
   "outputs": [],
   "source": [
    "adj_mat = pd.read_csv('adj_matrix_1000.csv')\n",
    "adj_mat = adj_mat.drop(columns=['Unnamed: 0'])\n",
    "adj_mat = adj_mat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 96033,
     "status": "ok",
     "timestamp": 1619819153645,
     "user": {
      "displayName": "LUCIA GARCIA - DUARTE SAENZ",
      "photoUrl": "",
      "userId": "14289334807482311885"
     },
     "user_tz": -120
    },
    "id": "MNT4w9B6EaAy"
   },
   "outputs": [],
   "source": [
    "feat_mat = pd.read_csv('feat_matrix.csv')\n",
    "feat_mat = feat_mat.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95728,
     "status": "ok",
     "timestamp": 1619819153645,
     "user": {
      "displayName": "LUCIA GARCIA - DUARTE SAENZ",
      "photoUrl": "",
      "userId": "14289334807482311885"
     },
     "user_tz": -120
    },
    "id": "GNMZIR5TIF5h",
    "outputId": "4d42f5a4-4d53-43a4-bcf7-b79f916028b1"
   },
   "outputs": [],
   "source": [
    "num_nodes, time_len = feat_mat.shape\n",
    "print(\"No. of stations:\", num_nodes, \"\\nNo. of timesteps:\", time_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7L-AU0QJ_93"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "Define functions to split the data into training, validation and test, to normalize the data, and to prepare the data to be fed into the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 94715,
     "status": "ok",
     "timestamp": 1619819153646,
     "user": {
      "displayName": "LUCIA GARCIA - DUARTE SAENZ",
      "photoUrl": "",
      "userId": "14289334807482311885"
     },
     "user_tz": -120
    },
    "id": "5arG3lpIrSYd"
   },
   "outputs": [],
   "source": [
    "def train_val_test_split(data, valid_portion, test_portion):\n",
    "    \n",
    "    time_len = data.shape[1]\n",
    "    train_portion = 1 - valid_portion - test_portion\n",
    "    train_size = int(time_len * train_portion)\n",
    "    valid_size = int(time_len * valid_portion)\n",
    "    \n",
    "    train_data = np.array(data.iloc[:, :train_size])\n",
    "    valid_data = np.array(data.iloc[:, train_size:train_size+valid_size])\n",
    "    test_data = np.array(data.iloc[:, train_size+valid_size:])\n",
    "    \n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "\n",
    "def scale_data(train_data, valid_data, test_data):\n",
    "    \n",
    "    max_speed = train_data.max()\n",
    "    min_speed = train_data.min()\n",
    "    \n",
    "    train_scaled = (train_data - min_speed) / (max_speed - min_speed)\n",
    "    valid_scaled = (valid_data - min_speed) / (max_speed - min_speed)\n",
    "    test_scaled = (test_data - min_speed) / (max_speed - min_speed)\n",
    "    \n",
    "    return train_scaled, valid_scaled, test_scaled\n",
    "\n",
    "\n",
    "def sequence_data_preparation(seq_len, pre_len, train_data, valid_data, test_data):\n",
    "    \n",
    "    trainX, trainY, validX, validY, testX, testY = [], [], [], [], [], []\n",
    "\n",
    "    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        a = train_data[:, i : i + seq_len + pre_len]\n",
    "        trainX.append(a[:, :seq_len])\n",
    "        trainY.append(a[:, -1])\n",
    "        \n",
    "    for i in range(valid_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        b = valid_data[:, i : i + seq_len + pre_len]\n",
    "        validX.append(b[:, :seq_len])\n",
    "        validY.append(b[:, -1])\n",
    "        \n",
    "    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        c = test_data[:, i : i + seq_len + pre_len]\n",
    "        testX.append(c[:, :seq_len])\n",
    "        testY.append(c[:, -1])\n",
    "\n",
    "    trainX = np.array(trainX)\n",
    "    trainY = np.array(trainY)\n",
    "    validX = np.array(validX)\n",
    "    validY = np.array(validY)\n",
    "    testX = np.array(testX)\n",
    "    testY = np.array(testY)\n",
    "\n",
    "    return trainX, trainY, validX, validY, testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Epfa0a36KCc5"
   },
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94005,
     "status": "ok",
     "timestamp": 1619819153646,
     "user": {
      "displayName": "LUCIA GARCIA - DUARTE SAENZ",
      "photoUrl": "",
      "userId": "14289334807482311885"
     },
     "user_tz": -120
    },
    "id": "3_hV2LmbrWYU",
    "outputId": "5256d35a-06b1-4e2a-b906-241ba296d770"
   },
   "outputs": [],
   "source": [
    "test_rate = 0.3\n",
    "valid_rate = 0.1\n",
    "train_data, valid_data, test_data = train_val_test_split(feat_mat, valid_rate, test_rate)\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Valid data: \", valid_data.shape)\n",
    "print(\"Test data: \", test_data.shape)\n",
    "feat_mat.shape[1] - train_data.shape[1] - valid_data.shape[1] - test_data.shape[1] # check dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yG-z-NPhKF7w"
   },
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 93324,
     "status": "ok",
     "timestamp": 1619819153647,
     "user": {
      "displayName": "LUCIA GARCIA - DUARTE SAENZ",
      "photoUrl": "",
      "userId": "14289334807482311885"
     },
     "user_tz": -120
    },
    "id": "YA4WeqjprZF9"
   },
   "outputs": [],
   "source": [
    "train_scaled, valid_scaled, test_scaled = scale_data(train_data, valid_data, test_data)\n",
    "#del feat_mat # to save memory for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzOQ_S1XKHxw"
   },
   "source": [
    "### Data preparation and reshaping\n",
    "- Each training observation are `seq_len` historical temperatures: the size of the past window of information\n",
    "- Each training prediction is the temperature X hours later (`pre_len`): how far in the future does the model need to learn to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93605,
     "status": "ok",
     "timestamp": 1619819154660,
     "user": {
      "displayName": "LUCIA GARCIA - DUARTE SAENZ",
      "photoUrl": "",
      "userId": "14289334807482311885"
     },
     "user_tz": -120
    },
    "id": "S2fsXLFgvvCU",
    "outputId": "c45e01d6-f231-434e-8384-6a77ec2a813a"
   },
   "outputs": [],
   "source": [
    "seq_len = 24    # change by 24, 24*7, 24*7*2, 24*7*3 (h)\n",
    "pre_len = 1     # change by 1, 2, 3 (h)\n",
    "\n",
    "trainX, trainY, validX, validY, testX, testY = sequence_data_preparation(\n",
    "    seq_len, pre_len, train_scaled, valid_scaled, test_scaled\n",
    ")\n",
    "\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(validX.shape)\n",
    "print(validY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94306,
     "status": "ok",
     "timestamp": 1619819155717,
     "user": {
      "displayName": "LUCIA GARCIA - DUARTE SAENZ",
      "photoUrl": "",
      "userId": "14289334807482311885"
     },
     "user_tz": -120
    },
    "id": "0fN3_ti9L9Hk",
    "outputId": "77da1980-76b2-4401-e031-a2f900fab7db"
   },
   "outputs": [],
   "source": [
    "gcn_lstm = GCN_LSTM(\n",
    "    seq_len=seq_len,\n",
    "    adj=adj_mat,\n",
    "    gc_layer_sizes=[16, 8],\n",
    "    gc_activations=[\"relu\", \"relu\"],\n",
    "    lstm_layer_sizes=[16, 16],           # change by 16, 32, 64, 128, 256\n",
    "    lstm_activations=[\"tanh\", \"tanh\"],\n",
    "    dropout = 0.2\n",
    ")\n",
    "\n",
    "x_input, x_output = gcn_lstm.in_out_tensors()\n",
    "model = Model(inputs=x_input, outputs=x_output)\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"mse\"])\n",
    "\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "#mc = ModelCheckpoint('models_GCN-LSTM/24_1_32_16/best_model.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQic1FxQnAKL"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "589o2n2rM0cC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "history = model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    #verbose=0,\n",
    "    validation_data=(validX, validY)#, callbacks = [mc]\n",
    ")\n",
    "end = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFtoY1QEu6Au"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Train loss: \",\n",
    "    history.history[\"loss\"][-1],\n",
    "    \"\\nValid loss:\",\n",
    "    history.history[\"val_loss\"][-1],\n",
    "    \"\\nElapsed time: \",\n",
    "    end - start\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEGLERhHvDGv"
   },
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##If loading best model\n",
    "#from tensorflow import keras\n",
    "#model=keras.models.load_model(\"models_GCN-LSTM/24_1_32_16/best_model.h5\")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60mkl37Fvady"
   },
   "outputs": [],
   "source": [
    "ythat = model.predict(trainX)\n",
    "yhat = model.predict(testX)\n",
    "yht = model.predict(validX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rescale values\n",
    "max_speed = train_data.max()\n",
    "min_speed = train_data.min()\n",
    "\n",
    "## actual train and test values\n",
    "train_rescref = np.array(trainY * max_speed)\n",
    "test_rescref = np.array(testY * max_speed)\n",
    "valid_rescref = np.array(validY * max_speed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rescpred = np.array((ythat) * max_speed)\n",
    "test_rescpred = np.array((yhat) * max_speed)\n",
    "valid_rescpred = np.array((yht) * max_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance measures\n",
    "\n",
    "from sklearn.metrics import r2_score, explained_variance_score#, mean_absolute_percentage_error\n",
    "from sklearn.metrics.regression import _check_reg_targets, check_consistent_length\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred,\n",
    "                                   sample_weight=None,\n",
    "                                   multioutput='uniform_average'):\n",
    "\n",
    "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
    "        y_true, y_pred, multioutput)\n",
    "    check_consistent_length(y_true, y_pred, sample_weight)\n",
    "    epsilon = np.finfo(np.float64).eps\n",
    "    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\n",
    "    output_errors = np.average(mape,\n",
    "                               weights=sample_weight, axis=0)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            return output_errors\n",
    "        elif multioutput == 'uniform_average':\n",
    "            # pass None as weights to np.average: uniform mean\n",
    "            multioutput = None\n",
    "\n",
    "    return np.average(output_errors, weights=multioutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naïve\n",
    "trainnpred = np.array(trainX)[\n",
    "    :, :, -1\n",
    "]  \n",
    "trainnpredc = (trainnpred) * max_speed\n",
    "\n",
    "# Metrics\n",
    "seg_nmael = [] #naïve mae\n",
    "seg_nmsel = [] #naïve mse\n",
    "seg_nmape = [] #naïve mape\n",
    "seg_nacc = []  #naïve accuracy\n",
    "seg_nR2 = []   #naïve coeff of determination\n",
    "seg_nvar = []  #naïve coeff of variance score\n",
    "\n",
    "seg_mael = []  #mae\n",
    "seg_masel = [] #mase\n",
    "seg_msel = []  #mse\n",
    "seg_mape = []  #mape\n",
    "seg_acc = []   #accuracy\n",
    "seg_R2 = []    #coeff of determination\n",
    "seg_var = []   #coeff of variance score\n",
    "\n",
    "kk=trainX.shape[1]\n",
    "for j in range(kk):\n",
    "    \n",
    "    ## NAIVE\n",
    "    # Mean Square Error \n",
    "    seg_nmsel.append(np.mean(np.square(train_rescref.T[j] - trainnpredc.T[j]))) \n",
    "        \n",
    "    # Mean Absolute Error \n",
    "    seg_nmael.append(np.mean(np.abs(train_rescref.T[j] - trainnpredc.T[j])))  \n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_nmape.append(mean_absolute_percentage_error(train_rescref.T[j],trainnpredc.T[j]))\n",
    "    \n",
    "    # Accuracy\n",
    "    seg_nacc.append(1-np.linalg.norm(train_rescref.T[j]-trainnpredc.T[j])/np.linalg.norm(train_rescref.T[j]))\n",
    "    \n",
    "    # Coefficient of determination\n",
    "    seg_nR2.append(r2_score(train_rescref.T[j],trainnpredc.T[j]))\n",
    "    \n",
    "    # Explained variance score\n",
    "    seg_nvar.append(explained_variance_score(train_rescref.T[j],trainnpredc.T[j]))\n",
    "    \n",
    "    \n",
    "    ## NN\n",
    "    # Mean Absolute Error \n",
    "    seg_mael.append(np.mean(np.abs(train_rescref.T[j] - train_rescpred.T[j])))  \n",
    "    \n",
    "    # Mean Square Error \n",
    "    seg_msel.append(np.mean(np.square(train_rescref.T[j] - train_rescpred.T[j]))) \n",
    "       \n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_mape.append(mean_absolute_percentage_error(train_rescref.T[j],train_rescpred.T[j]))\n",
    "    \n",
    "    # Accuracy\n",
    "    seg_acc.append(1-np.linalg.norm(train_rescref.T[j]-train_rescpred.T[j])/np.linalg.norm(train_rescref.T[j]))\n",
    "    \n",
    "    # Coefficient of determination\n",
    "    seg_R2.append(r2_score(train_rescref.T[j],train_rescpred.T[j]))\n",
    "    \n",
    "    # Explained variance score\n",
    "    seg_var.append(explained_variance_score(train_rescref.T[j],train_rescpred.T[j]))\n",
    "    \n",
    "    if seg_nmael[-1] != 0:\n",
    "        seg_masel.append(\n",
    "            seg_mael[-1] / seg_nmael[-1]\n",
    "        )  # Ratio of the two: Mean Absolute Scaled Error\n",
    "    else:\n",
    "        seg_masel.append(np.NaN)\n",
    "\n",
    "print(\"Total (ave) MAE for NN: \" + str(np.mean(np.array(seg_mael))))\n",
    "print(\"Total (ave) RMSE for NN: \" + str(np.mean(np.sqrt(np.array(seg_msel)))))\n",
    "print(\"Total (ave) MAPE for NN: \" + str(np.mean(np.array(seg_mape))))\n",
    "print(\"Total (ave) Accuracy for NN: \" + str(np.mean(np.array(seg_acc))))\n",
    "print(\"Total (ave) R2 for NN: \" + str(np.mean(np.array(seg_R2))))\n",
    "print(\"Total (ave) Var for NN: \" + str(np.mean(np.array(seg_var))))\n",
    "print(\"Total (ave) MASE for per-segment NN/naive MAE: \" + str(np.nanmean(np.array(seg_masel))))\n",
    "print(\"...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction.\")\n",
    "\n",
    "print(\"\\nTotal (ave) MAE for naive prediction: \" + str(np.mean(np.array(seg_nmael))))\n",
    "print(\"Total (ave) RMSE for naive prediction: \" + str(np.mean(np.sqrt(np.array(seg_nmsel)))))\n",
    "print(\"Total (ave) MAPE for naive prediction: \" + str(np.mean(np.array(seg_nmape))))\n",
    "print(\"Total (ave) Accuracy for naive prediction: \" + str(np.mean(np.array(seg_nacc))))\n",
    "print(\"Total (ave) R2 for naive prediction: \" + str(np.mean(np.array(seg_nR2))))\n",
    "print(\"Total (ave) Var for naive prediction: \" + str(np.mean(np.array(seg_nvar))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naïve\n",
    "validnpred = np.array(validX)[\n",
    "    :, :, -1\n",
    "] \n",
    "validnpredc = (validnpred) * max_speed\n",
    "\n",
    "# Metrics\n",
    "seg_nmael = [] #naïve mae\n",
    "seg_nmsel = [] #naïve mse\n",
    "seg_nmape = [] #naïve mape\n",
    "seg_nacc = []  #naïve accuracy\n",
    "seg_nR2 = []   #naïve coeff of determination\n",
    "seg_nvar = []  #naïve coeff of variance score\n",
    "\n",
    "seg_mael = []  #mae\n",
    "seg_masel = [] #mase\n",
    "seg_msel = []  #mse\n",
    "seg_mape = []  #mape\n",
    "seg_acc = []   #accuracy\n",
    "seg_R2 = []    #coeff of determination\n",
    "seg_var = []   #coeff of variance score\n",
    "\n",
    "kk=validX.shape[1]\n",
    "for j in range(kk):\n",
    "\n",
    "    ## NAIVE\n",
    "    # Mean Square Error\n",
    "    seg_nmsel.append(np.mean(np.square(valid_rescref.T[j] - validnpredc.T[j])))\n",
    "\n",
    "    # Mean Absolute Error\n",
    "    seg_nmael.append(np.mean(np.abs(valid_rescref.T[j] - validnpredc.T[j])))\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_nmape.append(mean_absolute_percentage_error(valid_rescref.T[j], validnpredc.T[j]))\n",
    "\n",
    "    # Accuracy\n",
    "    seg_nacc.append(1 - np.linalg.norm(valid_rescref.T[j] - validnpredc.T[j]) / np.linalg.norm(valid_rescref.T[j]))\n",
    "\n",
    "    # Coefficient of determination\n",
    "    seg_nR2.append(r2_score(valid_rescref.T[j], validnpredc.T[j]))\n",
    "\n",
    "    # Explained variance score\n",
    "    seg_nvar.append(explained_variance_score(valid_rescref.T[j], validnpredc.T[j]))\n",
    "\n",
    "    \n",
    "    ## NN\n",
    "    # Mean Absolute Error\n",
    "    seg_mael.append(np.mean(np.abs(valid_rescref.T[j] - valid_rescpred.T[j])))\n",
    "\n",
    "    # Mean Square Error\n",
    "    seg_msel.append(np.mean(np.square(valid_rescref.T[j] - valid_rescpred.T[j])))\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_mape.append(mean_absolute_percentage_error(valid_rescref.T[j], valid_rescpred.T[j]))\n",
    "\n",
    "    # Accuracy\n",
    "    seg_acc.append(1 - np.linalg.norm(valid_rescref.T[j] - valid_rescpred.T[j]) / np.linalg.norm(valid_rescref.T[j]))\n",
    "\n",
    "    # Coefficient of determination\n",
    "    seg_R2.append(r2_score(valid_rescref.T[j], valid_rescpred.T[j]))\n",
    "\n",
    "    # Explained variance score\n",
    "    seg_var.append(explained_variance_score(valid_rescref.T[j], valid_rescpred.T[j]))\n",
    "\n",
    "    if seg_nmael[-1] != 0:\n",
    "        seg_masel.append(seg_mael[-1] / seg_nmael[-1])  # Ratio of the two: Mean Absolute Scaled Error\n",
    "    else:\n",
    "        seg_masel.append(np.NaN)\n",
    "\n",
    "print(\"Total (ave) MAE for NN: \" + str(np.mean(np.array(seg_mael))))\n",
    "print(\"Total (ave) RMSE for NN: \" + str(np.mean(np.sqrt(np.array(seg_msel)))))\n",
    "print(\"Total (ave) MAPE for NN: \" + str(np.mean(np.array(seg_mape))))\n",
    "print(\"Total (ave) Accuracy for NN: \" + str(np.mean(np.array(seg_acc))))\n",
    "print(\"Total (ave) R2 for NN: \" + str(np.mean(np.array(seg_R2))))\n",
    "print(\"Total (ave) Var for NN: \" + str(np.mean(np.array(seg_var))))\n",
    "print(\"Total (ave) MASE for per-segment NN/naive MAE: \" + str(np.nanmean(np.array(seg_masel))))\n",
    "print(\"...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction.\")\n",
    "\n",
    "print(\"\\nTotal (ave) MAE for naive prediction: \" + str(np.mean(np.array(seg_nmael))))\n",
    "print(\"Total (ave) RMSE for naive prediction: \" + str(np.mean(np.sqrt(np.array(seg_nmsel)))))\n",
    "print(\"Total (ave) MAPE for naive prediction: \" + str(np.mean(np.array(seg_nmape))))\n",
    "print(\"Total (ave) Accuracy for naive prediction: \" + str(np.mean(np.array(seg_nacc))))\n",
    "print(\"Total (ave) R2 for naive prediction: \" + str(np.mean(np.array(seg_nR2))))\n",
    "print(\"Total (ave) Var for naive prediction: \" + str(np.mean(np.array(seg_nvar))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naïve\n",
    "testnpred = np.array(testX)[\n",
    "    :, :, -1\n",
    "] \n",
    "testnpredc = (testnpred) * max_speed\n",
    "\n",
    "# Metrics\n",
    "seg_nmael = [] #naïve mae\n",
    "seg_nmsel = [] #naïve mse\n",
    "seg_nmape = [] #naïve mape\n",
    "seg_nacc = []  #naïve accuracy\n",
    "seg_nR2 = []   #naïve coeff of determination\n",
    "seg_nvar = []  #naïve coeff of variance score\n",
    "\n",
    "seg_mael = []  #mae\n",
    "seg_masel = [] #mase\n",
    "seg_msel = []  #mse\n",
    "seg_mape = []  #mape\n",
    "seg_acc = []   #accuracy\n",
    "seg_R2 = []    #coeff of determination\n",
    "seg_var = []   #coeff of variance score\n",
    "\n",
    "kk=testX.shape[1]\n",
    "for j in range(kk):\n",
    "\n",
    "    ## NAIVE\n",
    "    # Mean Square Error\n",
    "    seg_nmsel.append(np.mean(np.square(test_rescref.T[j] - testnpredc.T[j])))\n",
    "\n",
    "    # Mean Absolute Error\n",
    "    seg_nmael.append(np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j])))\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_nmape.append(mean_absolute_percentage_error(test_rescref.T[j], testnpredc.T[j]))\n",
    "\n",
    "    # Accuracy\n",
    "    seg_nacc.append(1 - np.linalg.norm(test_rescref.T[j] - testnpredc.T[j]) / np.linalg.norm(test_rescref.T[j]))\n",
    "\n",
    "    # Coefficient of determination\n",
    "    seg_nR2.append(r2_score(test_rescref.T[j], testnpredc.T[j]))\n",
    "\n",
    "    # Explained variance score\n",
    "    seg_nvar.append(explained_variance_score(test_rescref.T[j], testnpredc.T[j]))\n",
    "\n",
    "    \n",
    "    ## NN\n",
    "    # Mean Absolute Error\n",
    "    seg_mael.append(np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j])))\n",
    "\n",
    "    # Mean Square Error\n",
    "    seg_msel.append(np.mean(np.square(test_rescref.T[j] - test_rescpred.T[j])))\n",
    "\n",
    "    # Mean Absolute Percentage Error\n",
    "    seg_mape.append(mean_absolute_percentage_error(test_rescref.T[j], test_rescpred.T[j]))\n",
    "\n",
    "    # Accuracy\n",
    "    seg_acc.append(1 - np.linalg.norm(test_rescref.T[j] - test_rescpred.T[j]) / np.linalg.norm(test_rescref.T[j]))\n",
    "\n",
    "    # Coefficient of determination\n",
    "    seg_R2.append(r2_score(test_rescref.T[j], test_rescpred.T[j]))\n",
    "\n",
    "    # Explained variance score\n",
    "    seg_var.append(explained_variance_score(test_rescref.T[j], test_rescpred.T[j]))\n",
    "\n",
    "    if seg_nmael[-1] != 0:\n",
    "        seg_masel.append(seg_mael[-1] / seg_nmael[-1])  # Ratio of the two: Mean Absolute Scaled Error\n",
    "    else:\n",
    "        seg_masel.append(np.NaN)\n",
    "\n",
    "print(\"Total (ave) MAE for NN: \" + str(np.mean(np.array(seg_mael))))\n",
    "print(\"Total (ave) RMSE for NN: \" + str(np.mean(np.sqrt(np.array(seg_msel)))))\n",
    "print(\"Total (ave) MAPE for NN: \" + str(np.mean(np.array(seg_mape))))\n",
    "print(\"Total (ave) Accuracy for NN: \" + str(np.mean(np.array(seg_acc))))\n",
    "print(\"Total (ave) R2 for NN: \" + str(np.mean(np.array(seg_R2))))\n",
    "print(\"Total (ave) Var for NN: \" + str(np.mean(np.array(seg_var))))\n",
    "print(\"Total (ave) MASE for per-segment NN/naive MAE: \" + str(np.nanmean(np.array(seg_masel))))\n",
    "print(\"...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction.\")\n",
    "\n",
    "print(\"\\nTotal (ave) MAE for naive prediction: \" + str(np.mean(np.array(seg_nmael))))\n",
    "print(\"Total (ave) RMSE for naive prediction: \" + str(np.mean(np.sqrt(np.array(seg_nmsel)))))\n",
    "print(\"Total (ave) MAPE for naive prediction: \" + str(np.mean(np.array(seg_nmape))))\n",
    "print(\"Total (ave) Accuracy for naive prediction: \" + str(np.mean(np.array(seg_nacc))))\n",
    "print(\"Total (ave) R2 for naive prediction: \" + str(np.mean(np.array(seg_nR2))))\n",
    "print(\"Total (ave) Var for naive prediction: \" + str(np.mean(np.array(seg_nvar))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'metrics/GCN_LSTM_24_1_32_16/'    #seq_len _ pre_len _ batch_size _ hidden_units\n",
    "\n",
    "a_file = open(mypath+\"MAE.txt\", \"w\")\n",
    "for row in np.matrix(seg_mael):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"RMSE.txt\", \"w\")\n",
    "for row in np.matrix(seg_msel):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"MASE.txt\", \"w\")\n",
    "for row in np.matrix(seg_masel):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"ACC.txt\", \"w\")\n",
    "for row in np.matrix(seg_acc):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"R2.txt\", \"w\")\n",
    "for row in np.matrix(seg_R2):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"EVS.txt\", \"w\")\n",
    "for row in np.matrix(seg_var):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Result visualization\n",
    "fig1 = plt.figure(figsize=(15, 8))\n",
    "#    ax1 = fig1.add_subplot(1,1,1)\n",
    "a_pred = test_rescpred[0:, 20]            # select one station (here 20)\n",
    "a_true = test_rescref[0:, 20]             # select one station (here 20)\n",
    "plt.plot(a_true, \"tab:green\", label=\"true\",linewidth=0.5)\n",
    "plt.plot(a_pred, \"tab:blue\", label=\"prediction\",linewidth=0.5)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"speed\")\n",
    "plt.legend(loc=\"best\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'models_GCN-LSTM/24_1_32_16/'    #seq_len _ pre_len _ batch_size _ hidden_units\n",
    "model.save(mypath + 'model.h5')\n",
    "\n",
    "a_file = open(mypath+\"train_rescref.txt\", \"w\")\n",
    "for row in np.matrix(train_rescref):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"train_rescpred.txt\", \"w\")\n",
    "for row in np.matrix(train_rescpred):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"valid_rescref.txt\", \"w\")\n",
    "for row in np.matrix(valid_rescref):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"valid_rescpred.txt\", \"w\")\n",
    "for row in np.matrix(valid_rescpred):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"test_rescref.txt\", \"w\")\n",
    "for row in np.matrix(test_rescref):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(mypath+\"test_rescpred.txt\", \"w\")\n",
    "for row in np.matrix(test_rescpred):\n",
    "    np.savetxt(a_file, row)\n",
    "a_file.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Graph Convolution + LSTM model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
